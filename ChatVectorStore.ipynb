{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef15c9e6-8072-4610-a6fe-fd226850c112",
   "metadata": {},
   "source": [
    "# Chatbot con un LLM utilizando Vector Stores - Interfaces de conversación \n",
    "\n",
    "En este cuaderno, vamos a adicionar funcionalidades que permitan enriquecer el contexto de los chatbot con Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a310c24-7fd7-4e2e-9a78-51def4ac04c9",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Una de las funcionalidades mas importantes cuando se construye un Chatbot es la capacidad de mantener el contexto de una conversación. Es importante que el LLM pueda mantener el hilo de la conversación.  \n",
    "\n",
    "## Tokens \n",
    "\n",
    "Son unidades de texto o codigo fuente que los LLMs utilizan para procesar y generar respuestas. Los Token pueden ser de diferentes tipos como: \n",
    "\n",
    "1. Palabras\n",
    "3. Caracteres\n",
    "4. Otros segmentos de texto o codigo\n",
    "\n",
    "Los token dependen entonces del metodo de tokenización utilizado. Se asigna un valor numerico o identificador y son organizados en secuencias o VECTORES que alimentan a los modelos y que de igual forma son su salida. \n",
    "\n",
    "\n",
    "## Embebidos (Embeddings) \n",
    "\n",
    "Los Embebidos son representaciones vectoriales de las palabras o tokens que capturan su significado semantico en un espacio dimensional ...... XD \n",
    "Mejor ...\n",
    "Los embebidos son la representación o codificación de los Tokens; algunos ejemplos pueden ser sentencias, parrafos o documentos completos puestos en un espacio vectorial de muchas dimensiones donde cada una de estas dimensiones representa una caracteristica o atributo aprendido del lenguaje. \n",
    "\n",
    "Son entonces, la forma en la cual los LLMs comprenden el significado y las relaciones que existen en el lenguaje. \n",
    "\n",
    "## FAISS \n",
    "\n",
    "Es una libreria que nos permite realizar busquedas por similitud o agrupamiento utilizando vectores de muchas dimensiones. En otras palabras FAISS va a permitir realizar busquedas sobre los Embebidos generados para este tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21bcdf90-5573-4c8d-afce-cce1f0ed2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54a8a47-81fb-4767-a114-801fb012019d",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "En el directorio data vas a encontrar un documento llamado constitucion-politica-col.pdf, en el vamos a encontrar tan solo 2 paginas con los 10 primeros artículos. \n",
    "\n",
    "## Nota \n",
    "Langchain ofrece diferentes herramientas para cargar datos o documentos, puedes dar un vistazo en el siguiente enlace \n",
    "[LangChainDocumentLoaders](https://python.langchain.com/docs/modules/data_connection/document_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5b12be-a70a-48f9-b2b8-9aa47a5685e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos cargados=2\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"data/constitucion-politica-col.pdf\")\n",
    "documents_col = loader.load() #\n",
    "print(f\"Documentos cargados={len(documents_col)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de931b-456e-4e91-84e3-8da8e750c626",
   "metadata": {},
   "source": [
    "Una vez hemos cargado los archivos debemos realizar una división del contenido para generar nuestros embeddings.\n",
    "No podemos cargar un solo cuerpo de texto para generar los embeddings, debemos separar el texto en pequeños fragmentos que nuestro LLM pueda soportar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9fdbe7-e7ff-45f9-8c7b-4dddf1d146d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = RecursiveCharacterTextSplitter(\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    "    chunk_size=200, \n",
    "    chunk_overlap=40\n",
    ").split_documents(documents_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ab4c7-6fff-470c-a718-4442c8da57ce",
   "metadata": {},
   "source": [
    "Vemos que Langchain nos ha ayudado a dividir nuestro gran cuerpo de texto en frases mas cortas que podemos enviar a nuestro generador de Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61788439-33bd-4504-8080-0a057b0ac76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos generados=23\n"
     ]
    }
   ],
   "source": [
    "print(f\"Documentos generados={len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd355c-7f57-4aa7-85df-758c46330f4c",
   "metadata": {},
   "source": [
    "## Instrucciones \n",
    "\n",
    "1. Alguien debe generar nuestros embeddigs, en nuestro ejemplo vamos a utilizar AWS Bedrock para generarlos y para lograrlo debemos conectarnos con el API de AWS\n",
    "2. Vamos a almacenar en memoria los datos generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d9a80c-146b-4dea-a8fb-3d37470a8202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorstore faiss con aws bedrock: elementos en el indice=23\n"
     ]
    }
   ],
   "source": [
    "vectorstore_faiss_aws = None\n",
    "try:\n",
    "    bedrock_embeddings = BedrockEmbeddings(\n",
    "        credentials_profile_name=\"default\", region_name=\"us-east-1\"\n",
    "    )\n",
    "    vectorstore_faiss_aws = FAISS.from_documents(\n",
    "        documents=docs,\n",
    "        embedding = bedrock_embeddings\n",
    "    )\n",
    "    print(f\"vectorstore faiss con aws bedrock: elementos en el indice={vectorstore_faiss_aws.index.ntotal}\")\n",
    "\n",
    "except ValueError as error:\n",
    "    print(f\"Error conectando con el vector store: {error}\")\n",
    "    raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b0f94-e5fa-44bc-8ccd-fbc5090f13c7",
   "metadata": {},
   "source": [
    "Hagamos una consulta sobre nuestro Vector Store con una busqueda por similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "324f1b60-7558-461d-84ab-727971410048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Documentos obtenidos: 4\n"
     ]
    }
   ],
   "source": [
    "query = \"Artículo 5\"\n",
    "retrieved_docs = vectorstore_faiss_aws.similarity_search(query)\n",
    "print(f\" Documentos obtenidos: {len(retrieved_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c044c-371f-412e-817f-59f657453f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: semantic search "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
